{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Definition of arguments for function usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1- Definition of arguments for function usage\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from flexiprune import *\n",
    "import argparse\n",
    "sys.argv = ['']\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Parameters for training')\n",
    "\n",
    "parser.add_argument('--model_architecture', type=str, default=\"VGG16\", \n",
    "                    help='Specify the architecture of the model (e.g., VGG16, AlexNet, etc.).')\n",
    "\n",
    "parser.add_argument('--method', type=str, default=\"weight\", \n",
    "                    help='Specify the training method (e.g., SenpisFaster, random, weight).')\n",
    "\n",
    "parser.add_argument('--dataset', type=str, default=\"CIFAR10\", \n",
    "                    help='Specify the dataset for training (e.g., CIFAR10, \"Name of custom dataset\").')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=8, \n",
    "                    help='Set the batch size for training.')\n",
    "\n",
    "parser.add_argument('--num_epochs', type=int, default=1, \n",
    "                    help='Specify the number of training epochs.')\n",
    "\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3, \n",
    "                    help='Set the learning rate for the optimizer.')\n",
    "\n",
    "parser.add_argument('--optimizer_val', type=str, default=\"SGD\", \n",
    "                    help='Specify the optimizer for training (e.g., SGD, Adam, etc.).')\n",
    "\n",
    "parser.add_argument('--model_type', type=str, default=\"UNPRUNED\", \n",
    "                    help='Specify the type of the model (e.g., PRUNED or UNPRUNED).')\n",
    "\n",
    "parser.add_argument('--device', type=str, default=None, \n",
    "                    help='Specify the device for training (e.g., \"cuda:0\" for GPU).')\n",
    "\n",
    "parser.add_argument('--model_input', default=torch.ones((1, 3, 224, 224)), \n",
    "                    help='Input tensor for the model (default is a tensor of ones).')\n",
    "\n",
    "parser.add_argument('--eval_metric', default=\"accuracy\", \n",
    "                    help='Specify the evaluation metric (e.g., accuracy, f1).')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=23, \n",
    "                    help='Set the seed for random pruning operations.')\n",
    "\n",
    "parser.add_argument('--list_pruning', type=list, \n",
    "                    default=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0], \n",
    "                    help='Specify the list of pruning ratios for each layer.')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.device is None:\n",
    "    import torch\n",
    "    args.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model, DATASET and TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(10, args)\n",
    "train_loader, test_loader, num_classes, trainset = get_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(train_loader = train_loader,\n",
    "            test_loader = test_loader,\n",
    "            model = model,\n",
    "            num_classes = num_classes,\n",
    "            args = args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_seeds = [23,42,1234]\n",
    "#distribution global PD = 50%\n",
    "dict_distri = {\"GPD-50\":\n",
    "               {\"PD1\" : [0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0],\n",
    "                \"PD2\" : [0.15,0.35,0.4,0.4,0.5,0.5,0.5,0.7,0.7,0.7,0.7,0.7,0.7,0.5,0.5,0],\n",
    "                \"PD3\" : [0.65,0.65,0.55,0.55,0.5,0.5,0.5,0.45,0.44,0.43,0.36,0.35,0.35,0.5,0.5,0],\n",
    "                \"PD4\" : [0.3,0.3,0.42,0.42,0.65,0.65,0.65,0.6,0.6,0.6,0.3,0.3,0.3,0.5,0.5,0],\n",
    "                \"PD5\" : [0.6,0.6,0.53,0.53,0.4,0.4,0.4,0.5,0.5,0.51,0.63,0.64,0.64,0.5,0.5,0]},\n",
    "                \"GPD-30\":\n",
    "                {\n",
    "                \"PD1\" : [0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0],\n",
    "                \"PD2\" : [0.15,0.15,0.2,0.2,0.3,0.3,0.3,0.43,0.43,0.44,0.45,0.45,0.46,0.3,0.3,0],\n",
    "                \"PD3\" : [0.45,0.45,0.35,0.35,0.3,0.3,0.3,0.25,0.25,0.25,0.16,0.16,0.13,0.3,0.3,0],\n",
    "                \"PD4\" : [0.15,0.15,0.3,0.3,0.45,0.45,0.45,0.28,0.28,0.27,0.2,0.19,0.15,0.3,0.3,0],\n",
    "                \"PD5\" : [0.4,0.4,0.2,0.2,0.3,0.3,0.3,0.3,0.3,0.3,0.37,0.37,0.36,0.3,0.3,0]},\n",
    "                \"GPD-20\":\n",
    "                {\n",
    "                \"PD1\":[0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2,0],\n",
    "                \"PD2\" : [0.15,0.15,0.17,0.17,0.2,0.2,0.2,0.22,0.22,0.22,0.3,0.3,0.31,0.2,0.2,0],\n",
    "                \"PD3\" : [0.35,0.35,0.25,0.25,0.2,0.2,0.2,0.13,0.13,0.12,0.1,0.1,0.09,0.2,0.2,0],\n",
    "                \"PD4\" : [0.15,0.15,0.15,0.15,0.34,0.34,0.34,0.15,0.15,0.15,0.1,0.09,0.09,0.2,0.2,0],\n",
    "                \"PD5\" : [0.35,0.35,0.2,0.2,0.11,0.11,0.1,0.19,0.2,0.2,0.31,0.31,0.31,0.2,0.2,0]}\n",
    "                }\n",
    "model_type = args.model_type\n",
    "\n",
    "for GPD in dict_distri:\n",
    "    for distri in dict_distri[GPD]:\n",
    "        args.list_pruning = dict_distri[GPD][distri]\n",
    "        for seed in list_seeds:\n",
    "            args.seed = seed\n",
    "            #original model unpruned\n",
    "            args.model_type = model_type\n",
    "            model = torch.load(f'models/{args.dataset}/{args.model_architecture}_{args.dataset}_{args.model_type}.pth')\n",
    "            model.to(args.device)\n",
    "            args.model_type = f'{distri}_{GPD}_PRUNED_SEED_{seed}'\n",
    "            #prune original model\n",
    "            prune_model(model,num_classes,trainset, args)\n",
    "            #retraining pruned model\n",
    "            args.model_type = f'{distri}_{GPD}_PRUNED_FT_SEED_{seed}'\n",
    "            train_model(\n",
    "                train_loader = train_loader,\n",
    "                test_loader = test_loader,\n",
    "                model = model,\n",
    "                num_classes = num_classes,\n",
    "                args = args)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "softwarexdistri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
